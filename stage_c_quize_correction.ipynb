{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Description Stability of the Grid System Electrical grids require a balance between electricity supply and demand in order to be stable. Conventional systems achieve this balance through demand-driven electricity production. For future grids with a high share of inflexible (i.e., renewable) energy source, the concept of demand response is a promising solution. This implies changes in electricity consumption in relation to electricity price changes. In this work, we’ll build a binary classification model to predict if a grid is stable or unstable using the UCI Electrical Grid Stability Simulated dataset.\n",
    "\n",
    "Dataset: https://archive.ics.uci.edu/ml/datasets/Electrical+Grid+Stability+Simulated+Data+\n",
    "\n",
    "It has 12 primary predictive features and two dependent variables.\n",
    "\n",
    "Predictive features:\n",
    "\n",
    "'tau1' to 'tau4': the reaction time of each network participant, a real value within the range 0.5 to 10 ('tau1' corresponds to the supplier node, 'tau2' to 'tau4' to the consumer nodes); 'p1' to 'p4': nominal power produced (positive) or consumed (negative) by each network participant, a real value within the range -2.0 to -0.5 for consumers ('p2' to 'p4'). As the total power consumed equals the total power generated, p1 (supplier node) = - (p2 + p3 + p4); 'g1' to 'g4': price elasticity coefficient for each network participant, a real value within the range 0.05 to 1.00 ('g1' corresponds to the supplier node, 'g2' to 'g4' to the consumer nodes; 'g' stands for 'gamma'); Dependent variables:\n",
    "\n",
    "'stab': the maximum real part of the characteristic differential equation root (if positive, the system is linearly unstable; if negative, linearly stable); 'stabf': a categorical (binary) label ('stable' or 'unstable'). Because of the direct relationship between 'stab' and 'stabf' ('stabf' = 'stable' if 'stab' <= 0, 'unstable' otherwise), 'stab' should be dropped and 'stabf' will remain as the sole dependent variable (binary classification).\n",
    "\n",
    "Split the data into an 80-20 train-test split with a random state of “1”. Use the standard scaler to transform the train set (x_train, y_train) and the test set (x_test). Use scikit learn to train a random forest and extra trees classifier. And use xgboost and lightgbm to train an extreme boosting model and a light gradient boosting model. Use random_state = 1 for training all models and evaluate on the test set.\n",
    "\n",
    "Also, to improve the Extra Trees Classifier, you will use the following parameters (number of estimators, minimum number of samples, minimum number of samples for leaf node and the number of features to consider when looking for the best split) for the hyperparameter grid needed to run a Randomized Cross Validation Search (RandomizedSearchCV).\n",
    "\n",
    "n_estimators = [50, 100, 300, 500, 1000]\n",
    "\n",
    "min_samples_split = [2, 3, 5, 7, 9]\n",
    "\n",
    "min_samples_leaf = [1, 2, 4, 6, 8]\n",
    "\n",
    "max_features = ['auto', 'sqrt', 'log2', None]\n",
    "\n",
    "hyperparameter_grid = {'n_estimators': n_estimators,\n",
    "\n",
    "                   'min_samples_leaf': min_samples_leaf,\n",
    "\n",
    "                   'min_samples_split': min_samples_split,\n",
    "\n",
    "                   'max_features': max_features}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In C:\\Users\\sky\\Anaconda4\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\sky\\Anaconda4\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\sky\\Anaconda4\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In C:\\Users\\sky\\Anaconda4\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\sky\\Anaconda4\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\sky\\Anaconda4\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\sky\\Anaconda4\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In C:\\Users\\sky\\Anaconda4\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tau1</th>\n",
       "      <th>tau2</th>\n",
       "      <th>tau3</th>\n",
       "      <th>tau4</th>\n",
       "      <th>p1</th>\n",
       "      <th>p2</th>\n",
       "      <th>p3</th>\n",
       "      <th>p4</th>\n",
       "      <th>g1</th>\n",
       "      <th>g2</th>\n",
       "      <th>g3</th>\n",
       "      <th>g4</th>\n",
       "      <th>stab</th>\n",
       "      <th>stabf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.959060</td>\n",
       "      <td>3.079885</td>\n",
       "      <td>8.381025</td>\n",
       "      <td>9.780754</td>\n",
       "      <td>3.763085</td>\n",
       "      <td>-0.782604</td>\n",
       "      <td>-1.257395</td>\n",
       "      <td>-1.723086</td>\n",
       "      <td>0.650456</td>\n",
       "      <td>0.859578</td>\n",
       "      <td>0.887445</td>\n",
       "      <td>0.958034</td>\n",
       "      <td>0.055347</td>\n",
       "      <td>unstable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>9.304097</td>\n",
       "      <td>4.902524</td>\n",
       "      <td>3.047541</td>\n",
       "      <td>1.369357</td>\n",
       "      <td>5.067812</td>\n",
       "      <td>-1.940058</td>\n",
       "      <td>-1.872742</td>\n",
       "      <td>-1.255012</td>\n",
       "      <td>0.413441</td>\n",
       "      <td>0.862414</td>\n",
       "      <td>0.562139</td>\n",
       "      <td>0.781760</td>\n",
       "      <td>-0.005957</td>\n",
       "      <td>stable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8.971707</td>\n",
       "      <td>8.848428</td>\n",
       "      <td>3.046479</td>\n",
       "      <td>1.214518</td>\n",
       "      <td>3.405158</td>\n",
       "      <td>-1.207456</td>\n",
       "      <td>-1.277210</td>\n",
       "      <td>-0.920492</td>\n",
       "      <td>0.163041</td>\n",
       "      <td>0.766689</td>\n",
       "      <td>0.839444</td>\n",
       "      <td>0.109853</td>\n",
       "      <td>0.003471</td>\n",
       "      <td>unstable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.716415</td>\n",
       "      <td>7.669600</td>\n",
       "      <td>4.486641</td>\n",
       "      <td>2.340563</td>\n",
       "      <td>3.963791</td>\n",
       "      <td>-1.027473</td>\n",
       "      <td>-1.938944</td>\n",
       "      <td>-0.997374</td>\n",
       "      <td>0.446209</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.929381</td>\n",
       "      <td>0.362718</td>\n",
       "      <td>0.028871</td>\n",
       "      <td>unstable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.134112</td>\n",
       "      <td>7.608772</td>\n",
       "      <td>4.943759</td>\n",
       "      <td>9.857573</td>\n",
       "      <td>3.525811</td>\n",
       "      <td>-1.125531</td>\n",
       "      <td>-1.845975</td>\n",
       "      <td>-0.554305</td>\n",
       "      <td>0.797110</td>\n",
       "      <td>0.455450</td>\n",
       "      <td>0.656947</td>\n",
       "      <td>0.820923</td>\n",
       "      <td>0.049860</td>\n",
       "      <td>unstable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9995</td>\n",
       "      <td>2.930406</td>\n",
       "      <td>9.487627</td>\n",
       "      <td>2.376523</td>\n",
       "      <td>6.187797</td>\n",
       "      <td>3.343416</td>\n",
       "      <td>-0.658054</td>\n",
       "      <td>-1.449106</td>\n",
       "      <td>-1.236256</td>\n",
       "      <td>0.601709</td>\n",
       "      <td>0.779642</td>\n",
       "      <td>0.813512</td>\n",
       "      <td>0.608385</td>\n",
       "      <td>0.023892</td>\n",
       "      <td>unstable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9996</td>\n",
       "      <td>3.392299</td>\n",
       "      <td>1.274827</td>\n",
       "      <td>2.954947</td>\n",
       "      <td>6.894759</td>\n",
       "      <td>4.349512</td>\n",
       "      <td>-1.663661</td>\n",
       "      <td>-0.952437</td>\n",
       "      <td>-1.733414</td>\n",
       "      <td>0.502079</td>\n",
       "      <td>0.567242</td>\n",
       "      <td>0.285880</td>\n",
       "      <td>0.366120</td>\n",
       "      <td>-0.025803</td>\n",
       "      <td>stable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9997</td>\n",
       "      <td>2.364034</td>\n",
       "      <td>2.842030</td>\n",
       "      <td>8.776391</td>\n",
       "      <td>1.008906</td>\n",
       "      <td>4.299976</td>\n",
       "      <td>-1.380719</td>\n",
       "      <td>-0.943884</td>\n",
       "      <td>-1.975373</td>\n",
       "      <td>0.487838</td>\n",
       "      <td>0.986505</td>\n",
       "      <td>0.149286</td>\n",
       "      <td>0.145984</td>\n",
       "      <td>-0.031810</td>\n",
       "      <td>stable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9998</td>\n",
       "      <td>9.631511</td>\n",
       "      <td>3.994398</td>\n",
       "      <td>2.757071</td>\n",
       "      <td>7.821347</td>\n",
       "      <td>2.514755</td>\n",
       "      <td>-0.966330</td>\n",
       "      <td>-0.649915</td>\n",
       "      <td>-0.898510</td>\n",
       "      <td>0.365246</td>\n",
       "      <td>0.587558</td>\n",
       "      <td>0.889118</td>\n",
       "      <td>0.818391</td>\n",
       "      <td>0.037789</td>\n",
       "      <td>unstable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9999</td>\n",
       "      <td>6.530527</td>\n",
       "      <td>6.781790</td>\n",
       "      <td>4.349695</td>\n",
       "      <td>8.673138</td>\n",
       "      <td>3.492807</td>\n",
       "      <td>-1.390285</td>\n",
       "      <td>-1.532193</td>\n",
       "      <td>-0.570329</td>\n",
       "      <td>0.073056</td>\n",
       "      <td>0.505441</td>\n",
       "      <td>0.378761</td>\n",
       "      <td>0.942631</td>\n",
       "      <td>0.045263</td>\n",
       "      <td>unstable</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          tau1      tau2      tau3      tau4        p1        p2        p3  \\\n",
       "0     2.959060  3.079885  8.381025  9.780754  3.763085 -0.782604 -1.257395   \n",
       "1     9.304097  4.902524  3.047541  1.369357  5.067812 -1.940058 -1.872742   \n",
       "2     8.971707  8.848428  3.046479  1.214518  3.405158 -1.207456 -1.277210   \n",
       "3     0.716415  7.669600  4.486641  2.340563  3.963791 -1.027473 -1.938944   \n",
       "4     3.134112  7.608772  4.943759  9.857573  3.525811 -1.125531 -1.845975   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9995  2.930406  9.487627  2.376523  6.187797  3.343416 -0.658054 -1.449106   \n",
       "9996  3.392299  1.274827  2.954947  6.894759  4.349512 -1.663661 -0.952437   \n",
       "9997  2.364034  2.842030  8.776391  1.008906  4.299976 -1.380719 -0.943884   \n",
       "9998  9.631511  3.994398  2.757071  7.821347  2.514755 -0.966330 -0.649915   \n",
       "9999  6.530527  6.781790  4.349695  8.673138  3.492807 -1.390285 -1.532193   \n",
       "\n",
       "            p4        g1        g2        g3        g4      stab     stabf  \n",
       "0    -1.723086  0.650456  0.859578  0.887445  0.958034  0.055347  unstable  \n",
       "1    -1.255012  0.413441  0.862414  0.562139  0.781760 -0.005957    stable  \n",
       "2    -0.920492  0.163041  0.766689  0.839444  0.109853  0.003471  unstable  \n",
       "3    -0.997374  0.446209  0.976744  0.929381  0.362718  0.028871  unstable  \n",
       "4    -0.554305  0.797110  0.455450  0.656947  0.820923  0.049860  unstable  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "9995 -1.236256  0.601709  0.779642  0.813512  0.608385  0.023892  unstable  \n",
       "9996 -1.733414  0.502079  0.567242  0.285880  0.366120 -0.025803    stable  \n",
       "9997 -1.975373  0.487838  0.986505  0.149286  0.145984 -0.031810    stable  \n",
       "9998 -0.898510  0.365246  0.587558  0.889118  0.818391  0.037789  unstable  \n",
       "9999 -0.570329  0.073056  0.505441  0.378761  0.942631  0.045263  unstable  \n",
       "\n",
       "[10000 rows x 14 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing our dataset\n",
    "df = pd.read_csv('Data_for_stagec_quize.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "Checking and Handling missing values.\n",
    "Splitting data into Training and test sets\n",
    "Feature Scaling (Standard scaling of data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tau1     0\n",
       "tau2     0\n",
       "tau3     0\n",
       "tau4     0\n",
       "p1       0\n",
       "p2       0\n",
       "p3       0\n",
       "p4       0\n",
       "g1       0\n",
       "g2       0\n",
       "g3       0\n",
       "g4       0\n",
       "stab     0\n",
       "stabf    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the dataset has no missing value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seperating our dataset into train and test using %20 for test with random_state=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['stab'])\n",
    "X = df.drop(columns=['stabf'])#you want to use thiscolsto be able tp predict stabf\n",
    "y = df['stabf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unstable    5092\n",
       "stable      2908\n",
       "Name: stabf, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling our dataset with standard scaler\n",
    "# Instantiating the Standard Scaler\n",
    "# Importing packages for Data preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "# fit and transform the training set\n",
    "X_train = pd.DataFrame(sc.fit_transform(X_train), columns=X_train.columns)\n",
    "\n",
    "# transfoorm the test set\n",
    "X_test = pd.DataFrame(sc.transform(X_test), columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#    quize questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1\n",
    "\n",
    "You are working on a spam classification system using regularized logistic regression. “Spam” is a positive class (y = 1) and “not spam” is the negative class (y = 0). You have trained your classifier and there are n = 2000 examples in the test set. The confusion matrix of predicted class vs. actual class is:\n",
    "\n",
    "What is the F1 score of this classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# answer = 0.3177"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3177"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "From the confusion matrix, we are given the following:\n",
    "TP = 355\n",
    "FP = 1480\n",
    "FN = 45\n",
    "TN = 120\n",
    "\"\"\"\n",
    "\n",
    "# precsion formula precision = TP/TP + FP \n",
    "\n",
    "precision = 355/(355 + 1480)\n",
    "\n",
    "# Calculate Recall with the formula TP/(TP + FN)\n",
    "recall = 355/(355 + 45)\n",
    "\n",
    "# compute the F1 score\n",
    "f1 = 2*(precision*recall)/(precision + recall)\n",
    "\n",
    "round(f1, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2\n",
    "\n",
    "Which method can we use to best fit a data in Logistic Regression?\n",
    "\n",
    "Answer maximum Likehood\n",
    "\n",
    "Question 3 Why do we use weak learners in boosting?\n",
    "\n",
    "Answer: To make the algorithm stronger\n",
    "\n",
    "Question 4\n",
    "\n",
    "A data scientist is evaluating different binary classification models. A false positive result is 5 times more expensive (from a business perspective) than a false negative result. The models should be evaluated based on the following criteria:\n",
    "\n",
    "1) Must have a recall rate of at least 80%\n",
    "\n",
    "2) Must have a false positive rate of 10% or less\n",
    "\n",
    "3) Must minimize business costs\n",
    "\n",
    "After creating each binary classification model, the data scientist generates the corresponding confusion matrix. Which confusion matrix represents the model that satisfies the requirements?\n",
    "\n",
    "Answer option B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall for option A is 0.78\n",
      "False Positive Rate for option A is 0.09\n",
      "Costs for option A is 67 \n",
      "\n",
      "Recall for option B is 0.82\n",
      "False Positive Rate for option B is 0.02\n",
      "Costs for option B is 28\n",
      "\n",
      "Recall for option C is 0.9\n",
      "False Positive Rate for option C is 0.04\n",
      "Costs for option C is 30 \n",
      "\n",
      "Recall for option D is 0.79\n",
      "False Positive Rate for option D is 0.01\n",
      "Costs for option D is 26 \n",
      "\n",
      "Option B meets all the requirements, hence it is the confusion metrix that represents the model\n"
     ]
    }
   ],
   "source": [
    "# Recall = TP / (TP + FN)\n",
    "# False Positive Rate (FPR) = FP / (FP + TN)\n",
    "# Cost = 5 * FP + FN\n",
    "\n",
    "# computing for option A\n",
    "print(f'Recall for option A is {78 / (78 + 22)}')\n",
    "print(f'False Positive Rate for option A is {9 / (9 + 91)}')\n",
    "print(f'Costs for option A is {5 * 9 + 22} \\n')\n",
    "\n",
    "# computing for option B\n",
    "print(f'Recall for option B is {82 / (82 + 18)}')\n",
    "print(f'False Positive Rate for option B is {2 / (2 + 98)}')\n",
    "print(f'Costs for option B is {5 * 2 + 18}\\n')\n",
    "\n",
    "# computing for option C\n",
    "print(f'Recall for option C is {90 / (90 + 10)}')\n",
    "print(f'False Positive Rate for option C is {4 / (4 + 96)}')\n",
    "print(f'Costs for option C is {5 * 4 + 10} \\n')\n",
    "\n",
    "# computing for option D\n",
    "print(f'Recall for option D is {79 / (79 + 21)}')\n",
    "print(f'False Positive Rate for option D is {round(1 / (1 + 91), 2)}')\n",
    "print(f'Costs for option D is {5 * 1 + 21} \\n')\n",
    "\n",
    "print('Option B meets all the requirements, hence it is the confusion metrix that represents the model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall for option A is 0.78\n",
    "False Positive Rate for option A is 0.09\n",
    "Costs for option A is 67 \n",
    "\n",
    "Recall for option B is 0.82\n",
    "False Positive Rate for option B is 0.02\n",
    "Costs for option B is 28\n",
    "\n",
    "Recall for option C is 0.9\n",
    "False Positive Rate for option C is 0.04\n",
    "Costs for option C is 30 \n",
    "\n",
    "Recall for option D is 0.79\n",
    "False Positive Rate for option D is 0.01\n",
    "Costs for option D is 26 \n",
    "\n",
    "Option B meets all the requirements, hence it is the confusion metrix that represents the model\n",
    "Question 5\n",
    "\n",
    "You are building a classifier and the accuracy is poor on both the training and test sets. Which would you use to try to improve the performance?\n",
    "\n",
    "Answer Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 6 Which of the following is not an Ensemble model?\n",
    "\n",
    "Answer DecisionTree\n",
    "\n",
    "Question 7 A classifier predicts if insurance claims are fraudulent or not. The cost of paying a fraudulent claim is higher than the cost of investigating a claim that is suspected to be fraudulent. Which metric should we use to evaluate this classifier?\n",
    "\n",
    "Answer Recall\n",
    "\n",
    "Question 8 The ROC curve above was generated from a classification algorithm. What can we say about this classifier?\n",
    "\n",
    "Answer The model has no descrimination capacity to distinguish between the postive and negative class.\n",
    "\n",
    "Question 9\n",
    "\n",
    "A random forest classifier was used to classify handwritten digits 0-9 into the numbers they were intended to represent. The confusion matrix below was generated from the results. Based on the matrix, which number was predicted with the least accuracy?\n",
    "\n",
    "Answer : 8\n",
    "\n",
    "Question 10\n",
    "\n",
    "A medical company is building a model to predict the occurrence of thyroid cancer. The training data contains 900 negative instances (people who don't have cancer) and 100 positive instances. The resulting model has 90% accuracy, but extremely poor recall. What steps can be used to improve the model's performance? (SELECT TWO OPTIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "Use bagging method\n",
    "Generate more samples/data using SMOTE\n",
    "Question 11 You are developing a machine learning classification algorithm that categorizes handwritten digits 0-9 into the numbers they represent. How should you pre-process the label data?\n",
    "\n",
    "Answer OneHotEncoding\n",
    "\n",
    "Question 12 What is the entropy of the target variable if its actual values are given as:\n",
    "\n",
    "[1,0,1,1,0,1,0]\n",
    "\n",
    "Answer Fromula for calculaying entropy is -Ep(x) *logp(x) hence the answer is -3/7log(3/7) - 4/7log(4/7)\n",
    "\n",
    "Question 13\n",
    "\n",
    "Which of this is not a good metric for evaluating classification algorithms for data with imbalanced class problems?\n",
    "\n",
    "Answer Accuracy\n",
    "\n",
    "Question 14\n",
    "\n",
    "What is the accuracy on the test set using the random forest classifier? In 4 decimal places.\n",
    "\n",
    "Answer As shown in the calculations below, the answer is 0.929"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages for modelling\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Importing packages for model evaluation\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, recall_score, precision_score, f1_score, roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of the RandomForestClassifier is 0.929\n"
     ]
    }
   ],
   "source": [
    "# instantiate the RandomForestClassifier with random_state of 1\n",
    "forest_clf = RandomForestClassifier(random_state=1)\n",
    "\n",
    "# Fit the classifier with training set\n",
    "forest_clf.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "forest_pred = forest_clf.predict(X_test)\n",
    "\n",
    "\n",
    "# compute the accuracy score\n",
    "acc_score = round(accuracy_score(y_test, forest_pred), 4)\n",
    "print(f'Accuracy score of the RandomForestClassifier is {acc_score}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 16\n",
    "\n",
    "What is the accuracy on the test set using the LGBM classifier? In 4 decimal places.\n",
    "\n",
    "Answer from the calculations below, the acuuracy score is 0.9290"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of the LGBMclassifier is 0.9375\n"
     ]
    }
   ],
   "source": [
    "# instantiate the LGBMClassifier with random_state of 1\n",
    "lgbm_clf = LGBMClassifier(random_state=1)\n",
    "\n",
    "# Fit the classifier with training set\n",
    "lgbm_clf.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "lgbm_pred = lgbm_clf.predict(X_test)\n",
    "\n",
    "# compute the accuracy score\n",
    "acc_score = round(accuracy_score(y_test, lgbm_pred), 4)\n",
    "print(f'Accuracy score of the LGBMclassifier is {acc_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 17\n",
    "\n",
    "To improve the Extra Trees Classifier, you will use the following parameters (number of estimators, minimum number of samples, minimum number of samples for leaf node and the number of features to consider when looking for the best split) for the hyperparameter grid needed to run a Randomized Cross Validation Search (RandomizedSearchCV).\n",
    "\n",
    "n_estimators = [50, 100, 300, 500, 1000]\n",
    "\n",
    "min_samples_split = [2, 3, 5, 7, 9]\n",
    "\n",
    "min_samples_leaf = [1, 2, 4, 6, 8]\n",
    "\n",
    "max_features = ['auto', 'sqrt', 'log2', None]\n",
    "\n",
    "hyperparameter_grid = {'n_estimators': n_estimators,\n",
    "\n",
    "                   'min_samples_leaf': min_samples_leaf,\n",
    "\n",
    "                   'min_samples_split': min_samples_split,\n",
    "\n",
    "                   'max_features': max_features}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the ExtraTreesClassifier as your estimator with cv=5, n_iter=10, scoring = 'accuracy', n_jobs = -1, verbose = 1 and random_state = 1. What are the best hyperparameters from the randomized search CV?\n",
    "\n",
    "Answer As shown in the ccomputations below, the answer is option B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:  3.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=ExtraTreesClassifier(random_state=1),\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'max_features': ['auto', 'sqrt', 'log2',\n",
       "                                                         None],\n",
       "                                        'min_samples_leaf': [1, 2, 4, 6, 8],\n",
       "                                        'min_samples_split': [2, 3, 5, 7, 9],\n",
       "                                        'n_estimators': [50, 100, 300, 500,\n",
       "                                                         1000]},\n",
       "                   scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "n_estimators = [50, 100, 300, 500, 1000]\n",
    "\n",
    "min_samples_split = [2, 3, 5, 7, 9]\n",
    "\n",
    "min_samples_leaf = [1, 2, 4, 6, 8]\n",
    "\n",
    "max_features = ['auto', 'sqrt', 'log2', None] \n",
    "\n",
    "# Define the param grid\n",
    "hyperparameter_grid = {'n_estimators': n_estimators,\n",
    "\n",
    "                       'min_samples_leaf': min_samples_leaf,\n",
    "\n",
    "                       'min_samples_split': min_samples_split,\n",
    "\n",
    "                       'max_features': max_features}\n",
    "\n",
    "# Instantiate the ExtratreClassifer\n",
    "ext_clf = ExtraTreesClassifier(random_state=1)\n",
    "\n",
    "# instantiate the gridsearch\n",
    "randomised_grid_search = RandomizedSearchCV(ext_clf, param_distributions= hyperparameter_grid,\n",
    "                                scoring='accuracy',\n",
    "                                n_iter=10,\n",
    "                                cv=5,\n",
    "                                n_jobs=-1,\n",
    "                                verbose=1)\n",
    "\n",
    "# fit the grid with training set\n",
    "randomised_grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 1000,\n",
       " 'min_samples_split': 2,\n",
       " 'min_samples_leaf': 1,\n",
       " 'max_features': 'log2'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the best hyperparameters\n",
    "randomised_grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 18\n",
    "\n",
    "Train a new ExtraTreesClassifier Model with the new Hyperparameters from the RandomizedSearchCV (with random_state = 1). Is the accuracy of the new optimal model higher or lower than the initial ExtraTreesClassifier model with no hyperparameter tuning?\n",
    "\n",
    "Answer After training a new extratree classifier with the new hperparameters, the accuracy score increased as shown below, hence the answer is HIGHER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old accuracy score is 0.928\n",
      "New Accuracy score is 0.934\n"
     ]
    }
   ],
   "source": [
    "# Compute the old accuracy\n",
    "ext_clf.fit(X_train, y_train)\n",
    "\n",
    "old_acc_score = round(accuracy_score(y_test, ext_clf.predict(X_test)), 4)\n",
    "print(f'old accuracy score is {old_acc_score}')\n",
    "\n",
    "\n",
    "# Get the model with new hyperparameters from the randomsearchcv\n",
    "new_model = randomised_grid_search.best_estimator_\n",
    "\n",
    "# Train a new ExtraTree model with new hyparameters\n",
    "new_model.fit(X_train, y_train)\n",
    "\n",
    "# make new predictions\n",
    "new_pred = new_model.predict(X_test)\n",
    "\n",
    "# Compute new accuracy score\n",
    "print(f'New Accuracy score is {round(accuracy_score(y_test, new_pred), 4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 19\n",
    "\n",
    "What other hyperparameter optimization methods can you try apart from Random Search?\n",
    "\n",
    "Answer GridSearch\n",
    "\n",
    "Question 20\n",
    "\n",
    "Find the feature importance using the optimal ExtraTreesClassifier model. Which features are the most and least important respectively?.\n",
    "\n",
    "Answer As shown from the plot below, the answer is tau2 and p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_importances = new_model.feature_importances_\n",
    "indices = np.argsort(feature_importances)[::-1]\n",
    "names = [X_train.columns[i] for i in indices]\n",
    "# Create plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Create plot title\n",
    "plt.title(\"Feature Importance\")\n",
    "\n",
    "# Add bars\n",
    "plt.bar(range(X_train.shape[1]), feature_importances[indices])\n",
    "\n",
    "# Add feature names as x-axis labels\n",
    "plt.xticks(range(X_train.shape[1]), names, rotation=90)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
